{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12cd9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Uses Stable-Baselines3 to train agents in the Knights-Archers-Zombies environment using SuperSuit vector envs.\n",
    "\n",
    "This environment requires using SuperSuit's Black Death wrapper, to handle agent death.\n",
    "\n",
    "For more information, see https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "\n",
    "Author: Elliot (https://github.com/elliottower)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy, MlpPolicy\n",
    "\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "# from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "\n",
    "\n",
    "def train(env_fn, steps: int = 100, seed: int | None = 0, **env_kwargs):\n",
    "    # Train a single model to play as each agent in an AEC environment\n",
    "    env = env_fn.parallel_env(**env_kwargs)\n",
    "\n",
    "    # Add black death wrapper so the number of agents stays constant\n",
    "    # MarkovVectorEnv does not support environments with varying numbers of active agents unless black_death is set to True\n",
    "    env = ss.black_death_v3(env)\n",
    "\n",
    "    # Pre-process using SuperSuit\n",
    "    visual_observation = False #not env.unwrapped.continuous_actions not env.unwrapped.vector_state\n",
    "    if visual_observation:\n",
    "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
    "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "        env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
    "    env = ss.multiagent_wrappers.pad_observations_v0(env)\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "    # Use a CNN policy if the observation space is visual\n",
    "    model = PPO(\n",
    "        CnnPolicy if visual_observation else MlpPolicy,\n",
    "        env,\n",
    "        verbose=3,\n",
    "        batch_size=256,\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=steps)\n",
    "\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c206e453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on simple_tag_v3.\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aron_\\.conda\\envs\\deeplearning\\lib\\site-packages\\pettingzoo\\utils\\conversions.py:252: UserWarning: The base environment `simple_tag_v3` does not have a `render_mode` defined.\n",
      "  warnings.warn(\n",
      "C:\\Users\\aron_\\.conda\\envs\\deeplearning\\lib\\site-packages\\pettingzoo\\utils\\conversions.py:132: UserWarning: The base environment `aec_observation_lambda<simple_tag_v3>` does not have a `render_mode` defined.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 4477  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 14    |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "Model has been saved.\n",
      "Finished training on simple_tag_v3.\n"
     ]
    }
   ],
   "source": [
    "# Set vector_state to false in order to use visual observations (significantly longer training time)\n",
    "env_kwargs = dict(num_good=1, num_adversaries=3, num_obstacles=2, max_cycles=25, continuous_actions=False )\n",
    "# max_cycles=100, max_zombies=4, vector_state=True\n",
    "# Train a model (takes ~5 minutes on a laptop CPU)\n",
    "env_fn = simple_tag_v3\n",
    "train(env_fn, steps=10000, seed=0, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15eb0f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval(env_fn, num_games: int = 10000, render_mode: str | None = None, **env_kwargs):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "    # Pre-process using SuperSuit\n",
    "    # if visual_observation = False #not env.unwrapped.continuous_actions\n",
    "    if visual_observation:\n",
    "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
    "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "        env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "    print(\n",
    "        f\"\\nStarting evaluation on {str(env.metadata['name'])} (num_games={num_games}, render_mode={render_mode})\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "\n",
    "    model = PPO.load(latest_policy)\n",
    "\n",
    "    rewards = {agent: 0 for agent in env.possible_agents}\n",
    "\n",
    "    # Note: we evaluate here using an AEC environments, to allow for easy A/B testing against random policies\n",
    "    # For example, we can see here that using a random agent for archer_0 results in less points than the trained agent\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "        env.action_space(env.possible_agents[0]).seed(i)\n",
    "        \n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "            if agent == 'agent_0':\n",
    "                obs=np.append(obs, [0,0])\n",
    "            #print(obs)\n",
    "            if render_mode== 'human':\n",
    "                time.sleep(0.01)\n",
    "            for agent in env.agents:\n",
    "                rewards[agent] += env.rewards[agent]\n",
    "\n",
    "            if termination or truncation:\n",
    "                break\n",
    "            else:\n",
    "                if agent == env.possible_agents[0]:\n",
    "                    act = env.action_space(agent).sample()\n",
    "                else:\n",
    "                    act = model.predict(obs, deterministic=True)[0]\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    avg_reward = sum(rewards.values()) / len(rewards.values())\n",
    "    avg_reward_per_agent = {\n",
    "        agent: rewards[agent] / num_games for agent in env.possible_agents\n",
    "    }\n",
    "    print(f\"Avg reward: {avg_reward}\")\n",
    "    print(\"Avg reward per agent, per game: \", avg_reward_per_agent)\n",
    "    print(\"Full rewards: \", rewards)\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set vector_state to false in order to use visual observations (significantly longer training time)\n",
    "env_kwargs = dict(num_good=1, num_adversaries=3, num_obstacles=2, max_cycles=25, continuous_actions=False)\n",
    "eval(env_fn, num_games=10, render_mode=None, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a3c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy, MlpPolicy\n",
    "\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "env_fn = simple_tag_v3\n",
    "print(env_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set vector_state to false in order to use visual observations (significantly longer training time)\n",
    "env_kwargs = dict(num_good=1, num_adversaries=3, num_obstacles=2, max_cycles=200, continuous_actions=False )\n",
    "eval(env_fn, 100, 'human', **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad686a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
